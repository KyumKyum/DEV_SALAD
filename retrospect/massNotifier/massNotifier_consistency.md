# 대용량 알림 처리 서버 개발 회고록 3: 분산 클러스터 환경에서 직면한 문제 - 데이터 연속성]

### 개요 (TL;DR)
#### 개발한 것: 대형 알림 처리 마이크로 서비스 (TPS 10K ~ 1M)
- 메세지 큐와 이벤트 스트리밍 구조를 활용한 대용량 트래픽 처리 서비스 개발 경험
#### 직면했던 문제 상황: Data Consistency
- 하나의 거대한 Shared Event Store가 아닌, Distributed Cache일 시 일어나는 데이터 연속성 문제.
#### 해결 방법: 
- IMDB(`Redis`) Cluster로 구현을 하였던 Event Store을 IMDG(`Hazelcast`)로 변경
---
### 배경
- 분산 클러스터 환경일 시, Stash & Flush 전략을 사용할 시 사용되는 Event Store 역시 분산의 대상일 것이다.
- 만약 하나의 Redis로 구성이 된 Event Store가 분산된 여러개의 캐시로 변경될 시, 분명 데이터 연속성에 문제가 생길 것이다.
    - 캐시 1,2,3이 있다고 가정을 했을 때, 이벤트들은 1,2,3에 각각 저장이 될 것이고, 이는 데이터가 연속적으로 되고 합쳐지는데 큰 문제가 있을 것이다. 큐의 FIFO 속성으로 인해 지켜지던 데이터의 순서가 보장되지 않는 것이다.
    - 이를 지키기 위해서는 각 위치의 인덱스를 저장한 후에, 모든 캐시를 순회해야지 완성할 수 있다는 것인데, 그렇게 되면 효율성이 떨어져 속도가 빠른 Redis를 사용한다는 점이 퇴색이 된다.
---
### 문제 분석
- **Improper Shared Cache**
    - Stash & Flush가 일어나는 캐시의 경우, 스케일 아웃이 어렵다는 문제도 존재한다. 데이터베이스 샤딩과 비슷한 문제이고, 이를 위해 샤딩 프로세스를 구현하기 위해 Hash Ring을 구현하는 것은 오버 엔지니어링일 것이다. 
        - 현재 구조에서 Event Store은 공유가 되는 하나의 캐시여야할텐데, 과연 최대 100만건의 요청을 해당 캐시가 버틸 수 있을지는 의문이다.
    - 하나의 공유된 캐시를 달아도 문제가 될 수도 있는 것이, 여러개의 consumer는 이 역시 먼저 가져와서 각각의 메세지를 만들려고 할 것이다. 즉, 이 문제도 Race Condition과 관련이 되어있다.

---
### 아이디어들
1. Redis Cluster (IMDB Cluster)
- Redis로 클러스터를 구성해서 하나의 Master와 여러 개의 Slave를 만들어 분산 환경을 구성하려고 했다. Redis 클러스터는 다음과 같은 플로우를 갖는다.
    1. Master에게 데이터를 요청한다.
    2. 만약에 데이터가 있으면 데이터를 반환한다.
    3. 데이터가 없으면 인덱스 정보를 조회해 어느 Slave에 있는지 확인하고, 그 정보를 반환한다.
    4. 반환된 정보로 Slave에 접근하여 정보를 가져온다.
- 하지만 레디스 클러스터로 구현할 시, 다음과 같은 점이 우려가 되었다.
    1. Master에 데이터가 없으면 결국 데이터베이스를 한번 더 찔러야 한다는 점.
    2. 로컬 클러스터 환경이 k8s에서는 어떤 복잡도를 야기할지 예상이 어렵다는 점.
    3. Near cache와 같이 효용성을 높이기 위한 기술이 Redis에서는 Paid-service 라는 점.

2. IMDG (In-Memory Data Grid)
- 조금 더 기술조사를 해보니, IMDB (In-Memory Database)보다 현재 상황에 더욱 맞는 기술을 찾았다!
- 인메모리 데이터 그리드는 **각 노드의 메모리를 공유하여 모든 서비스가 함께 사용할 수 있도록 하는 메모리 클러스터**를 이야기한다. 즉, 애초에 분산 환경을 위해 만들어진 개념이고, 여러 서비스의 공유된 캐시를 위해서 니온 개념이라고 한다!
- 고가용성(High-availability)과 확장성을 제공하는 분산 메모리 시스템은 다음과 같은 특징을 가졌다.
    - Auto-scaling (Scale Out / Down) & managment 
    - 객체 지향형 데이터 관리 (이거 진짜 마음에 들더라)
    - 디스크와 DBMS와 연결하여 데이터 영속성 관리 가능
- 현재 상황에 필요한 기술이기에 관련된게 무엇이 있을까 조사를 해보고, 가장 개발 공수가 적게 들 것 같은 `Hazelcast`로 결정을 했다.
---
### 해결:  Hazelcast
- `Hazelcast`는 도커 이미지가 있기에 바로 docker-compose로 받아서 띄웠다.
- 따로 Datagrip에서 지원하지는 않지만, `Hazelcast`에서 자체적으로 지원하는 GUI Monitoring & Managerment Tool이 있어서 그것 역시 docker-compose로 받아서 띄웠다.
- Cache라는 이름을 EventStore로 변경한 후 (이벤트를 캐싱하는 용도로만 사용하여 바꾸는 것으로 판단을 하였다), 곧바로 `Hazelcast`로 이식을 하였다.
- `jemeter`로 테스트를 진행, 50000만건의 테스트에 대해서 모두 데이터가 깨지지 않는 것을 확인하였다! 😎😎
