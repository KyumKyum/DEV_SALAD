# 대용량 알림 처리 서버 개발 회고록 2: 분산 클러스터 환경에서 직면한 문제들

### 개요 (TL;DR)
#### 개발한 것: 대형 알림 처리 마이크로 서비스 (TPS 10K ~ 1M)
- 메세지 큐와 이벤트 스트리밍 구조를 활용한 대용량 트래픽 처리 서비스 개발 경험
#### 직면했던 문제 상황: Race Condition
- 분산 클러스터 환경 속 이벤트 스트리밍 구조에서 발생하는 Consumer Race Conditon
- 하나의 Shared Cache와, 여러개의 Worker로 인해 발생하는 Publisher/Consumer Race Condition
---
### 배경
- **분산 클러스터 환경을 생각하지 못했다.**
- 순간 머릿속에 들어온 '분산 클러스터 환경'은 분명 내가 고려하지 못한 부분이었다.
- 곧바로 pm2로 인스턴스 3개를 만들어 테스트 환경을 만들어봤고, 요청을 보내자 예상했던대로 다음과 같은 문제가 발생했다.
    1. 요청이 중복이 되어 응답이 된다.
        - 요청은 1개만 보냈는데, 응답은 3개가 오는 상황이 발생했다. 
        - 로드 벨런싱 문제는 아닐것이다. pm2 자체에서 로드 밸런싱이 되는 것으로 알고 있고, `pm2 monit`으로 되는 것을 확인하였다. 추가로 nginx로 간단하게 로드 벨런서를 달았는데도 상황은 똑같았다.
    2. 캐시에서 가져오는 데이터의 연속성이 망가지게 되었다.
        - 여러개의 응답 중에서, Stash & Flush를 사용하는 요청의 경우, 나누어진 '이벤트'들의 순서가 엉망이 되는 경우를 발견했다.

### 문제 분석
1. 중복 응답: **Race Condition**
    - 로그를 찍어보니 분명 하나의 메세지만 publish가 되었는데, 인스턴스의 수 만큼 consume이 되어 로그에 찍히는 것이다. Consumer간 Race Condition이 일어나는 것 같다는 생각이 들었다.
        - 나중에 서술하겠지만, publisher에서도 Race Condition이 일어나고 있다. Consumer 문제를 어느정도 해결을 하니 일부 인스턴스의 publisher가 (가장 최근에 메세지를 publish했던 publisher) 동시에 메세지를 publish하는 것을 발견했다.
    - 서버는 여러대가 되었지만, 캐시는 1개이다. 데이터의 연속성을 지키기 위해서 각 서버가 해당 cache에 접근하는 순서와 조건이 제한이 되어야하지만, 그런 것이 전혀 없었다.
        
2. 망가진 연속성: **Improper Shared Cache**
    - Stash & Flush가 일어나는 캐시의 경우, 스케일 아웃이 어렵다는 문제도 존재한다. 데이터베이스 샤딩과 비슷한 문제이고, 이를 위해 샤딩 프로세스를 구현하기 위해 Hash Ring을 구현하는 것은 오버 엔지니어링일 것이다. 
        - 결국 해당 캐시는 공유가 되는 하나의 캐시여야할텐데, 과연 최대 100만건의 요청을 해당 캐시가 버틸 수 있을지는 의문이다.
    - 하나의 공유된 캐시를 달아도 문제가 될 수도 있는 것이, 여러개의 consumer는 이 역시 먼저 가져와서 각각의 메세지를 만들려고 할 것이다. 즉, 이 문제도 Race Condition과 관련이 되어있다.
---
### 해결: 중복 응답
#### 1. Instance Key based channel.
- 기존에는 채널 이름을 `enum`으로 정의해서 사용했기에, 여러개의 인스턴스가 형성되었을 시 여러 대의 subscriber가 같은 채널로 구독을 하는 상황이 만들어져 Race Condition이 만들어지게 되었다.
- `instanceKey`를 정의하여 (uuid 기반의 키) 각 인스턴스마다 배정을 하고, 채널명 + `instanceKey`가 채널명이 되게 해서 각 인스턴스의 채널이 독립되게 만들었다.
    - e.g. 인스턴스 1의 `instanceKey`가 abc라고 하고, 인스턴스 2의 `instanceKey`가 def라고 가정할 시, `QUEUE_MEDIUM`이라는 채널의 이름은 인스턴스 1에게는 `QUEUE_MEDIUM:adc`, 인스턴스 2에게는 `QUEUE_MEDIUM:def`가 되게 된다.
- 각 채널이 분리되고, 이제는 Consumer는 각각이 독립된 채널을 구독하게 되면서 Race가 발생하지 않게 되었다. Clustered Payload인 경우에도 Load Balance는 request의 단위로 인스턴스를 결정하니, 전달되는 이벤트들의 연속성에도 문제가 없게 되었다.
- Jmeter로 1000스레드로 50건 요청을 날렸을 시, 50000건 정확하게 도착하는 것을 확인했다. 이 정도면, MVP레벨에서는 해결이군!

----
### 해결: 망가진 연속성
#### . Bulk Consume 방법으로 결정.(설명 추가)